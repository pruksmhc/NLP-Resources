{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import openai\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import regex as re\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from openai.embeddings_utils import get_embedding\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tabulate import tabulate\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import List\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_QA = pd.read_csv(\"wikipedia_question_similar_answer.tsv\", names=[\"question\", \"wikipedia_answer\"], skip_blank_lines=\"skip\")\n",
    "QA_df = pd.read_json(\"~/Downloads/gooaq_pairs.jsonl\", lines=True)\n",
    "QA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "for idx, row in QA_df.iloc[200:4000].iterrows():\n",
    "    question = row[0]\n",
    "    answer = row[1]\n",
    "    embedding_from_string(string=question, embedding_cache = embeddings) \n",
    "    embedding_from_string(string=answer, embedding_cache = embeddings) \n",
    "embeddings =  {key: torch.tensor(emb) for key, emb in embeddings.items()}\n",
    "embedding_cache_file = 'qa_pairs_embedding.pkl'\n",
    "pickle.dump(embeddings, open(embedding_cache_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(n_splits=1, train_size=.90, random_state=42)\n",
    "groups = QA_df[0]\n",
    "train_indices, test_indices = next(splitter.split(QA_df, groups=groups))\n",
    "\n",
    "train_df = QA_df.iloc[train_indices]\n",
    "test_df = QA_df.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = adding_negative_pairs(train_df)\n",
    "test_df = adding_negative_pairs(test_df)\n",
    "train_label_dict = [{\"query\": row[0], \"doc\": row[1], \"label\": row['label']} for _, row in train_df.iterrows()]\n",
    "test_label_dict = [{\"query\": row[0], \"doc\": row[1], \"label\": row['label']} for _, row in test_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(train_label_dict, open('train.pkl', 'wb'))\n",
    "#pickle.dump(test_label_dict, open('test.pkl', 'wb'))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "embeddings = pickle.load(open('qa_pairs_embedding.pkl', 'rb'))\n",
    "train_list = pickle.load(open('train.pkl', 'rb'))\n",
    "test_list = pickle.load(open('test.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_resources.finetune_openai_embeddings import dataset\n",
    "from nlp_resources.finetune_openai_embeddings.dataset import RetrievalDataset\n",
    "\n",
    "train_dataloader = DataLoader(RetrievalDataset(embeddings, train_list), batch_size=32)\n",
    "test_dataloader = DataLoader(RetrievalDataset(embeddings, test_list), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_resources.finetune_openai_embeddings.models import SimilarityModel\n",
    "\n",
    "model = SimilarityModel(embedding_dim=1536, hidden_dim=768, out_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8422it [00:46, 180.37it/s]\n",
      "8422it [00:13, 606.62it/s]\n",
      "/Users/yadapruk/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/multiclass.py:322: RuntimeWarning: invalid value encountered in cast\n",
      "  if y.dtype.kind == \"f\" and np.any(y != y.astype(int)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d0/02n8hcmn2wv568m0wsxrtvhm0000gp/T/ipykernel_9599/2938109933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnlp_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune_openai_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/nlp_resources/finetune_openai_embeddings/train_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, test_dataloader, device, model, num_epochs, ratio_pos_neg)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         table = {\"Epoch\": [epoch],\n",
      "\u001b[0;32m~/Desktop/nlp_resources/finetune_openai_embeddings/train_utils.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(train_dataloader, device, model, optimizer, epoch, save_path, writer, pos_weight)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# print(f\"   Training accuracy is {accuracy:.4f}, AUC is {auc:.4f}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/nlp_resources/finetune_openai_embeddings/train_utils.py\u001b[0m in \u001b[0;36m_evaluation\u001b[0;34m(writer, test_dataloader, device, model, epoch, pos_weight)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_rpa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss/Epoch(test)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \"\"\"\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mtype_of_target\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"f\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"continuous\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "from nlp_resources.finetune_openai_embeddings.train_utils import train\n",
    "\n",
    "train(train_dataloader, test_dataloader, device, model, num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrino_wiki = \"\"\"\n",
    "A neutrino (/njuːˈtriːnoʊ/ new-TREE-noh; denoted by the Greek letter ν) is a fermion (an elementary particle with spin of 1/2) that interacts only via the weak interaction and gravity.[2][3] The neutrino is so named because it is electrically neutral and because its rest mass is so small (-ino) that it was long thought to be zero. The rest mass of the neutrino is much smaller than that of the other known elementary particles excluding massless particles.[1] The weak force has a very short range, the gravitational interaction is extremely weak due to the very small mass of the neutrino, and neutrinos do not participate in the strong interaction.[4] Thus, neutrinos typically pass through normal matter unimpeded and undetected.[\n",
    "\n",
    "Weak interactions create neutrinos in one of three leptonic flavors: electron neutrinos (νe), muon neutrinos (νμ), or tau neutrinos (ντ), in association with the corresponding charged lepton.[5] Although neutrinos were long believed to be massless, it is now known that there are three discrete neutrino masses with different tiny values (the smallest of which may even be zero[6]), but the three masses do not uniquely correspond to the three flavors. A neutrino created with a specific flavor is a specific mixture of all three mass states (a quantum superposition). \n",
    "\n",
    "Similar to some other neutral particles, neutrinos oscillate between different flavors in flight as a consequence. For example, an electron neutrino produced in a beta decay reaction may interact in a distant detector as a muon or tau neutrino.[7][8] The three mass values are not yet known as of 2022, but laboratory experiments and cosmological observations have determined the differences of their squares,[9] an upper limit on their sum (< 2.14×10−37 kg),[1][10] and an upper limit on the mass of the electron neutrino.\n",
    "\n",
    "For each neutrino, there also exists a corresponding antiparticle, called an antineutrino, which also has spin of 1/2 and no electric charge. Antineutrinos are distinguished from neutrinos by having opposite-signed lepton number and weak isospin, and right-handed instead of left-handed chirality. To conserve total lepton number (in nuclear beta decay), electron neutrinos only appear together with positrons (anti-electrons) or electron-antineutrinos, whereas electron antineutrinos only appear with electrons or electron neutrinos.[12][13]\n",
    "\n",
    "Neutrinos are created by various radioactive decays; the following list is not exhaustive, but includes some of those processes: beta decay of atomic nuclei or hadrons,.natural nuclear reactions such as those that take place in the core of a star.artificial nuclear reactions in nuclear reactors, nuclear bombs, or particle acceleratorsduring a supernova. during the spin-down of a neutron star. when cosmic rays or accelerated particle beams strike atoms.\n",
    "\n",
    "The majority of neutrinos which are detected about the Earth are from nuclear reactions inside the Sun. At the surface of the Earth, the flux is about 65 billion (6.5×1010) solar neutrinos, per second per square centimeter.[14][15] Neutrinos can be used for tomography of the interior of the earth.[16][17]\n",
    "\n",
    "Machine learning (ML) is a field of inquiry devoted to understanding and building methods that \"learn\" – that is, methods that leverage data to improve performance on some set of tasks.[1] It is seen as a part of artificial intelligence.\n",
    "\n",
    "Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.[3][4]\n",
    "\n",
    "A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[6][7]\n",
    "\n",
    "Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.[8][9]In its application across business problems, machine learning is also referred to as predictive analytics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question1 = \"What are neutrinos?\"\n",
    "test_question2 = \"How are neutrinos created?\"\n",
    "# sentences = [line.strip() for line in re.split(r' *[\\.\\n]+', sterile_neutrino_wiki) if line]\n",
    "paragraphs = [line.strip() for line in re.split(r' *[\\n]+', neutrino_wiki) if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(emb_path, \"wb\") as embedding_cache_file\n",
    "embedding_cache_file = 'qa_pairs_embedding.pkl'\n",
    "pickle.dump(embeddings, open(embedding_cache_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = TwoTowerModelRecall(embedding_dim=1536, hidden_dim=768, out_dim=256).to(device)\n",
    "\n",
    "# state_dict = torch.load(\"/mnt/ruian/test/wikiQA/training-0.0001-20230305-060248/335-model.pt\")\n",
    "state_dict = torch.load(\"/mnt/ruian/test/wikiQA/training-0.0001-20230305-062353/181-model.pt\")\n",
    "\n",
    "test_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(test_question):\n",
    "    test_model.eval()\n",
    "\n",
    "#     q_emb = torch.tensor(embeddings[test_question])\n",
    "    q_emb = embeddings[test_question].clone().detach()\n",
    "    q_emb = q_emb.unsqueeze(0)\n",
    "\n",
    "    rows = []\n",
    "    for paragraph in paragraphs:\n",
    "#         p_emb = torch.tensor(embeddings[paragraph])\n",
    "        p_emb = embeddings[paragraph].clone().detach()\n",
    "        p_emb = p_emb.unsqueeze(0)\n",
    "\n",
    "        q_emb, p_emb = q_emb.to(device), p_emb.to(device)\n",
    "        output = torch.sigmoid(test_model(q_emb, p_emb))\n",
    "\n",
    "        row = {\"question\": test_question, \"paragraph\": paragraph, \"score\": output.item()}\n",
    "        rows.append(row)\n",
    "        \n",
    "    qa_output = pd.DataFrame(rows)\n",
    "    qa_output = qa_output.sort_values(by=\"score\", ascending=False)\n",
    "    return qa_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer(test_question1)\n",
    "# second highest is the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_answer(test_question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_row(row):\n",
    "    model.eval()\n",
    "    q_emb = torch.tensor(embeddings[row['question']])\n",
    "    q_emb = q_emb.unsqueeze(0)\n",
    "    \n",
    "    p_emb = torch.tensor(embeddings[row['wikipedia_answer']])\n",
    "    p_emb = p_emb.unsqueeze(0)\n",
    "\n",
    "    q_emb, p_emb = q_emb.to(device), p_emb.to(device)\n",
    "    output = torch.sigmoid(model(q_emb, p_emb))\n",
    "\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA_df['score'] = QA_df.apply(inference_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['infer_score'] = train_df.apply(inference_row, axis=1)\n",
    "test_df['infer_score'] = test_df.apply(inference_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_row(data):\n",
    "    model.eval()\n",
    "    q_emb = torch.tensor(embeddings[row['question']])\n",
    "    q_emb = q_emb.unsqueeze(0)\n",
    "    \n",
    "    p_emb = torch.tensor(embeddings[row['wikipedia_answer']])\n",
    "    p_emb = p_emb.unsqueeze(0)\n",
    "\n",
    "    q_emb, p_emb = q_emb.to(device), p_emb.to(device)\n",
    "    output = torch.sigmoid(model(q_emb, p_emb))\n",
    "\n",
    "    return output.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
